{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0265ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ccc948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "assert HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eb7937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666135aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SearchConfig:\n",
    "    # model to be loaded from huggingface\n",
    "    model_id: str\n",
    "    # number of candidates\n",
    "    population_size: int\n",
    "    # generations\n",
    "    num_generations: int\n",
    "    # how many best programs to add to the LLM context for generation\n",
    "    # in paper terms, inspiration to the LLM\n",
    "    num_parent_context: int\n",
    "\n",
    "\n",
    "search_config = SearchConfig(\n",
    "    model_id=\"google/gemma-2b-it\",\n",
    "    population_size=5,\n",
    "    num_generations=10,\n",
    "    num_parent_context=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567f466a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Program:\n",
    "    \"\"\"\n",
    "    Represents a candidate solution (an 'Individual' in evolutionary terms).\n",
    "    AlphaEvolve stores these in a Program Database.\n",
    "    \"\"\"\n",
    "\n",
    "    code: str\n",
    "    # initial score to inf\n",
    "    # requires cuda (since torch inf isn't defined for cpus)\n",
    "    fitness: float = -float(\"inf\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Program(fitness={self.fitness:.4f})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001d64fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    \"\"\"\n",
    "    The automated evaluator that assigns a scalar score to code.\n",
    "    In this demo, we want the agent to discover the function: f(x) = x^2 + 2x + 1\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: make this extensible, instead of being hardcoded\n",
    "    def __init__(self):\n",
    "        # Ground truth data (x, y) pairs\n",
    "        self.test_inputs = [-5, -2, 0, 2, 5, 10]\n",
    "        self.test_targets = [x**2 + 2 * x + 1 for x in self.test_inputs]\n",
    "\n",
    "    def evaluate(self, code_str: str) -> float:\n",
    "        \"\"\"\n",
    "        Executes the code securely (mocked here with exec) and calculates error.\n",
    "        Higher fitness is better (fitness = -error).\n",
    "        \"\"\"\n",
    "        # Define a local scope to run the generated code\n",
    "        local_scope = {}\n",
    "\n",
    "        try:\n",
    "            # TODO: find an alternative to exec, should be fine for\n",
    "            # offline runs though!\n",
    "            exec(code_str, {}, local_scope)\n",
    "\n",
    "            # We expect the LLM to define a function named 'solve'\n",
    "            if \"solve\" not in local_scope:\n",
    "                return -float(\"inf\")\n",
    "\n",
    "            candidate_func = local_scope[\"solve\"]\n",
    "\n",
    "            # Calculate Mean Squared Error\n",
    "            total_error = 0\n",
    "            for x, target in zip(self.test_inputs, self.test_targets):\n",
    "                prediction = candidate_func(x)\n",
    "                if not isinstance(prediction, (int, float)):\n",
    "                    return -float(\"inf\")\n",
    "                total_error += (prediction - target) ** 2\n",
    "\n",
    "            # Return negative error (maximization problem)\n",
    "            return -total_error\n",
    "\n",
    "        except Exception:\n",
    "            # Code that crashes gets the lowest fitness\n",
    "            return -float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23111355",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaEvolveAgent:\n",
    "    def __init__(self, config: SearchConfig):\n",
    "        self.config = config\n",
    "\n",
    "        print(f\"Loading {config.model_id}...\")\n",
    "        # Load Gemma on GPU\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_id)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.config.model_id,\n",
    "            device_map=\"auto\",\n",
    "            # TODO: make this a param\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "        self.evaluator = Evaluator()\n",
    "        self.population: List[Program] = []\n",
    "\n",
    "    def seed_population(self, initial_code: str):\n",
    "        \"\"\"Initialize the database with a user-provided starting point[cite: 60].\"\"\"\n",
    "        fitness = self.evaluator.evaluate(initial_code)\n",
    "        self.population.append(Program(code=initial_code, fitness=fitness))\n",
    "        print(f\"Seeded with fitness: {fitness}\")\n",
    "\n",
    "    def construct_prompt(self, parent: Program, inspirations: List[Program]) -> str:\n",
    "        \"\"\"\n",
    "        Builds the 'Rich Context' prompt[cite: 65].\n",
    "        It includes 'Prior programs' (inspirations) and the 'Current program' (parent) to mutate.\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Context: Show high-performing past solutions\n",
    "        prompt_content = \"You are an intelligent coding assistant. Your goal is to optimize a Python function to match a hidden mathematical pattern.\\n\\n\"\n",
    "\n",
    "        if inspirations:\n",
    "            prompt_content += \"--- Prior Best Solutions ---\\n\"\n",
    "            for p in inspirations:\n",
    "                prompt_content += f\"Score: {p.fitness}\\nCode:\\n{p.code}\\n\\n\"\n",
    "\n",
    "        # 2. Task: Present the parent code to modify\n",
    "        prompt_content += \"--- Current Code to Improve ---\\n\"\n",
    "        prompt_content += f\"{parent.code}\\n\\n\"\n",
    "\n",
    "        prompt_content += \"--- Task ---\\n\"\n",
    "        prompt_content += \"Rewrite the 'Current Code' to improve its accuracy. \"\n",
    "        prompt_content += \"Think about the pattern in the Prior Solutions. \"\n",
    "        prompt_content += \"Output ONLY the full Python code for the 'solve' function. No markdown, no explanation.\"\n",
    "\n",
    "        # Format for Gemma (Chat Template)\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt_content}]\n",
    "        return self.tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "    def extract_code(self, llm_response: str) -> str:\n",
    "        \"\"\"Parses the LLM output to extract executable Python code.\"\"\"\n",
    "        # Simple regex to find python code blocks if the model uses markdown\n",
    "        match = re.search(r\"```python\\n(.*?)\\n```\", llm_response, re.DOTALL)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "\n",
    "        # If no markdown, assume the whole response is code (fallback)\n",
    "        # Cleaning up common chat artifacts\n",
    "        clean_code = llm_response.replace(\"```\", \"\").strip()\n",
    "        return clean_code\n",
    "\n",
    "    # TODO: implement some form of early stopping in case fitness doesn't\n",
    "    # improve after a fixed number of steps\n",
    "    @torch.no_grad()\n",
    "    def llm_mutate(self, parent: Program, inspirations: List[Program]) -> str:\n",
    "        \"\"\"\n",
    "        Uses the LLM to propose a 'diff' or rewrite of the parent code.\n",
    "        \"\"\"\n",
    "        prompt = self.construct_prompt(parent, inspirations)\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.7,  # High temp for diversity/exploration\n",
    "            do_sample=True,\n",
    "        )\n",
    "\n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # Remove the prompt from the output to get just the response\n",
    "        response_text = generated_text[\n",
    "            len(self.tokenizer.decode(inputs.input_ids[0], skip_special_tokens=True)) :\n",
    "        ]\n",
    "\n",
    "        return self.extract_code(response_text)\n",
    "\n",
    "    def step(self, generation_idx):\n",
    "        \"\"\"Runs one iteration of the evolutionary loop.\"\"\"\n",
    "        print(f\"\\n--- Generation {generation_idx} ---\")\n",
    "\n",
    "        # Sort population by fitness (descending)\n",
    "        self.population.sort(key=lambda p: p.fitness, reverse=True)\n",
    "\n",
    "        # Keep top K as \"Inspirations\" for the prompt (Elitism)\n",
    "        inspirations = self.population[: self.config.num_parent_context]\n",
    "\n",
    "        new_programs = []\n",
    "\n",
    "        # Generate offspring\n",
    "        # We take the best parent and try to mutate it multiple times\n",
    "        parent = self.population[0]\n",
    "\n",
    "        for i in range(self.config.population_size):\n",
    "            print(f\"  > Mutating parent (Fitness: {parent.fitness})...\", end=\"\")\n",
    "\n",
    "            try:\n",
    "                # 1. LLM Mutation\n",
    "                mutated_code = self.llm_mutate(parent, inspirations)\n",
    "\n",
    "                # 2. Evaluation\n",
    "                fitness = self.evaluator.evaluate(mutated_code)\n",
    "                print(f\" Result Fitness: {fitness}\")\n",
    "\n",
    "                # 3. Add to pool\n",
    "                new_programs.append(Program(code=mutated_code, fitness=fitness))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\" Failed: {e}\")\n",
    "\n",
    "        # Update Population (Join and Select)\n",
    "        self.population.extend(new_programs)\n",
    "        self.population.sort(key=lambda p: p.fitness, reverse=True)\n",
    "        # Prune to fixed size, keeping only the best ones in terms of fitness\n",
    "        self.population = self.population[: self.config.population_size]\n",
    "\n",
    "        print(f\"Best in Gen {generation_idx}: {self.population[0].fitness}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ebf2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting seed program\n",
    "initial_heuristic = \"\"\"\n",
    "def solve(x):\n",
    "# Initial guess: linear relationship\n",
    "return x * 2\n",
    "\"\"\"\n",
    "\n",
    "agent = AlphaEvolveAgent(search_config)\n",
    "agent.seed_population(initial_heuristic)\n",
    "\n",
    "for gen in range(1, search_config.num_generations + 1):\n",
    "    agent.step(gen)\n",
    "\n",
    "print(\"\\n=== Final Discovered Solution ===\")\n",
    "print(agent.population[0].code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
